{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672a3e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install Dependencies\n",
    "!pip install transformers datasets peft trl torch accelerate bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63009eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"unbiased-news-summarizer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596623a5",
   "metadata": {},
   "source": [
    "## Stage 1: Supervised Fine-Tuning (SFT)\n",
    "First, we train the model to be a good summarizer using the Multi-News dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abade562",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import Dataset\n",
    "# Load Dataset\n",
    "\n",
    "DOC_COL = \"document\"\n",
    "REF_COL = \"summary\"\n",
    "dataset_train = load_dataset(\"Awesome075/multi_news_parquet\", split=\"train\").to_pandas()\n",
    "dataset_test = load_dataset(\"Awesome075/multi_news_parquet\", split=\"test\").to_pandas()\n",
    "dataset_val = load_dataset(\"Awesome075/multi_news_parquet\", split=\"validation\").to_pandas()\n",
    "dataset_train[DOC_COL].replace('', np.nan, inplace=True)\n",
    "dataset_train[REF_COL].replace('', np.nan, inplace=True)\n",
    "dataset_train.dropna(inplace=True)\n",
    "\n",
    "dataset_train = Dataset.from_pandas(dataset_train)\n",
    "dataset_val = Dataset.from_pandas(dataset_val)\n",
    "dataset_test = Dataset.from_pandas(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02acf0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model & Tokenizer (Example: BART-large)\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train = dataset_train.map(preprocess_function, batched=True)\n",
    "tokenized_val = dataset_val.map(preprocess_function, batched=True)\n",
    "\n",
    "# Define Trainer (Standard HuggingFace Trainer)\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# Initialize the Data Collator explicitly\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./sft_summarizer\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"sft_summarizer_run\",\n",
    "    logging_steps=500,\n",
    "    logging_strategy=\"steps\",\n",
    "    save_strategy=\"steps\", # Explicitly set save strategy\n",
    "    save_steps=5000,       # Save every 1000 steps (less frequent)\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17c17b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"./sft_summarizer_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329ef95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 2. Reward Model Wrapper\n",
    "class NeutralityRewardModel(torch.nn.Module):\n",
    "    def __init__(self, reward_model_name, policy_tokenizer, device):\n",
    "        super().__init__()\n",
    "        self.reward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_name).to(device)\n",
    "        self.reward_tokenizer = AutoTokenizer.from_pretrained(reward_model_name)\n",
    "        self.policy_tokenizer = policy_tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
    "        texts = self.policy_tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "        inputs = self.reward_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.reward_model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1)\n",
    "        k = min(3, probs.shape[-1])\n",
    "        top_probs, _ = torch.topk(probs, k, dim=-1)\n",
    "        avg_top_probs = top_probs.mean(dim=-1)\n",
    "        rewards = (1.0 - avg_top_probs) ** 2\n",
    "        return rewards\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "reward_model = NeutralityRewardModel(\"maximuspowers/bias-type-classifier\", tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d05283",
   "metadata": {},
   "source": [
    "## Stage 2: DPO Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c7618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# STRATEGY 2: Offline Preference Learning (DPO)\n",
    "# ==========================================\n",
    "\n",
    "# STEP 1: Generate Preference Dataset\n",
    "# We generate 2 responses for each prompt, score them, and create (chosen, rejected) pairs.\n",
    "\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Load Models\n",
    "sft_model_path = \"./unbiased_summarizer_dpo_final_1000\"\n",
    "\n",
    "# Policy Model (The actor)\n",
    "policy_model = AutoModelForSeq2SeqLM.from_pretrained(sft_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(sft_model_path)\n",
    "\n",
    "\n",
    "def create_preference_dataset(model, tokenizer, reward_model, source_dataset, num_samples=100, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Generates a dataset for DPO training.\n",
    "    Returns a HuggingFace Dataset with columns: ['prompt', 'chosen', 'rejected']\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    data_rows = []\n",
    "    \n",
    "    print(f\"Generating {num_samples} preference pairs...\")\n",
    "    \n",
    "    # Iterate through the source dataset\n",
    "    # Assuming source_dataset has a 'document' column\n",
    "    for i in tqdm(range(min(num_samples, len(source_dataset)))):\n",
    "        doc = source_dataset[i]['document']\n",
    "        \n",
    "        # 1. Generate 2 responses\n",
    "        inputs = tokenizer(doc, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=128,\n",
    "                min_length=30,\n",
    "                do_sample=True,\n",
    "                num_beams=1,\n",
    "                top_p=0.95, # Slightly higher temp/p for diversity\n",
    "                num_return_sequences=2,\n",
    "                early_stopping=False\n",
    "            )\n",
    "        \n",
    "        print(\"scoring\")\n",
    "        # 2. Score them\n",
    "        rewards = reward_model(outputs) # [2]\n",
    "        scores = rewards.tolist()\n",
    "        \n",
    "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        \n",
    "        # 3. Determine Winner\n",
    "        if scores[0] > scores[1]:\n",
    "            chosen = decoded[0]\n",
    "            rejected = decoded[1]\n",
    "            margin = scores[0] - scores[1]\n",
    "        else:\n",
    "            chosen = decoded[1]\n",
    "            rejected = decoded[0]\n",
    "            margin = scores[1] - scores[0]\n",
    "            \n",
    "        # Only keep pairs where there is a meaningful difference? \n",
    "        # For now, we keep all to maximize data, could filter by margin > 0.01\n",
    "        data_rows.append({\n",
    "            \"prompt\": doc,\n",
    "            \"chosen\": chosen,\n",
    "            \"rejected\": rejected,\n",
    "            \"score_margin\": margin\n",
    "        })\n",
    "\n",
    "        # Save every 1000 iterations\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            df = pd.DataFrame(data_rows)\n",
    "            df.to_csv(save_path, index=False)\n",
    "            print(f\"Saved {len(data_rows)} pairs to {save_path}\")\n",
    "            \n",
    "    # Final save\n",
    "    if len(data_rows) > 0:\n",
    "        df = pd.DataFrame(data_rows)\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"Final save: {len(data_rows)} pairs to {save_path}\")\n",
    "        \n",
    "    return Dataset.from_list(data_rows)\n",
    "\n",
    "# Generate the dataset (using a small subset for demonstration)\n",
    "# We use the 'dataset_train' loaded earlier\n",
    "preference_dataset = create_preference_dataset(\n",
    "    policy_model, \n",
    "    tokenizer, \n",
    "    reward_model, \n",
    "    dataset_train, \n",
    "    num_samples=10000, # Adjust this number based on time constraints\n",
    "    device=policy_model.device\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(preference_dataset)} pairs.\")\n",
    "print(\"Sample Pair:\")\n",
    "print(preference_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a49bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Train with DPO\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# RELOAD MODEL: Ensure we have a fresh, trainable copy of the SFT model\n",
    "# This prevents the \"None of the inputs have requires_grad=True\" error\n",
    "print(\"Reloading SFT model for DPO...\")\n",
    "sft_model_path = \"./sft_summarizer_final\"\n",
    "policy_model = AutoModelForSeq2SeqLM.from_pretrained(sft_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(sft_model_path)\n",
    "\n",
    "policy_model.config.decoder_start_token_id = tokenizer.eos_token_id\n",
    "policy_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "policy_model.generation_config.decoder_start_token_id = tokenizer.eos_token_id\n",
    "policy_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# We need a fresh copy of the model for DPO to avoid PPO artifacts if any\n",
    "# Or we can continue fine-tuning the SFT model.\n",
    "# Ideally, DPO requires a reference model (the SFT model) and a policy model (initialized from SFT).\n",
    "num_samples = 1000\n",
    "\n",
    "print(\"Setting up DPO Trainer...\")\n",
    "\n",
    "# 1. Config\n",
    "dpo_config = DPOConfig(\n",
    "    output_dir=\"./unbiased_summarizer_dpo\",\n",
    "    learning_rate=5e-6,             # Low learning rate for DPO\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=20,             # Short training for demo\n",
    "    beta=0.1,                       # The beta parameter for DPO (KL penalty)\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# 2. Initialize Trainer\n",
    "# Note: DPOTrainer automatically creates a reference model copy if not provided,\n",
    "# but providing it explicitly is safer if memory allows.\n",
    "# Since we are in a notebook, we might want to rely on the implicit copy or free memory first.\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=policy_model,             # The model to train\n",
    "    ref_model=None,                 # TRL will create a copy of 'model' as reference\n",
    "    args=dpo_config,\n",
    "    train_dataset=preference_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    # DPO expects specific column names, which we matched in creation\n",
    ")\n",
    "\n",
    "print(\"Starting DPO Training...\")\n",
    "dpo_trainer.train()\n",
    "if hasattr(dpo_trainer.model, \"generation_config\"):\n",
    "    dpo_trainer.model.generation_config.length_penalty = 1.0\n",
    "\n",
    "dpo_trainer.save_model(f\"./unbiased_summarizer_dpo_final_{num_samples}\")\n",
    "print(\"DPO Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac16a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# STRATEGY 1: Online \"Best-of-N\" Inference\n",
    "# ==========================================\n",
    "# This strategy generates N candidates at inference time and selects the one \n",
    "# with the highest neutrality score.\n",
    "\n",
    "# 1. Load Models\n",
    "sft_model_path = \"./sft_summarizer_final\"\n",
    "\n",
    "# Policy Model (The actor)\n",
    "policy_model = AutoModelForSeq2SeqLM.from_pretrained(sft_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(sft_model_path)\n",
    "\n",
    "def generate_best_of_n(model, tokenizer, reward_model, input_text, n=4, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Prepare Input\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "    \n",
    "    # 2. Generate N candidates\n",
    "    # We use sampling to ensure diversity among the N candidates\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            min_length=30,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            num_beams=1,\n",
    "            num_return_sequences=n,\n",
    "            early_stopping=False\n",
    "        )\n",
    "    \n",
    "    # 3. Decode candidates\n",
    "    candidates = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "    # 4. Score candidates\n",
    "    # We need to tokenize candidates for the reward model\n",
    "    # The reward model expects input_ids. We'll use the reward_tokenizer inside the wrapper if accessible,\n",
    "    # or just re-use the main tokenizer if they are compatible.\n",
    "    # Based on previous cells, reward_model is an instance of NeutralityRewardModel.\n",
    "    \n",
    "    candidate_scores = []\n",
    "    for cand in candidates:\n",
    "        # The NeutralityRewardModel expects input_ids of the *summary* (or text, depending on implementation).\n",
    "        # Looking at the implementation in Cell 8, it takes input_ids, decodes them, then re-tokenizes for the classifier.\n",
    "        # So we can pass the generated output ids directly.\n",
    "        pass\n",
    "    \n",
    "    # Let's use the reward model's internal logic directly to be safe\n",
    "    # We'll pass the raw output IDs to the reward model's forward method\n",
    "    # The reward model forward expects a batch.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # outputs is [n, seq_len]\n",
    "        rewards = reward_model(outputs) # Returns tensor of shape [n]\n",
    "        \n",
    "    best_idx = torch.argmax(rewards).item()\n",
    "    best_candidate = candidates[best_idx]\n",
    "    best_score = rewards[best_idx].item()\n",
    "    \n",
    "    return {\n",
    "        \"best_summary\": best_candidate,\n",
    "        \"best_score\": best_score,\n",
    "        \"all_candidates\": candidates,\n",
    "        \"all_scores\": rewards.tolist()\n",
    "    }\n",
    "\n",
    "# --- Test Strategy 1 ---\n",
    "sample_text = \"\"\"\n",
    "The controversial bill was passed yesterday amid fierce protests. Critics argue it undermines democracy, while supporters claim it is necessary for national security. The opposition leader called it a \"dark day,\" whereas the Prime Minister hailed it as a \"historic victory.\"\n",
    "\"\"\"\n",
    "\n",
    "result = generate_best_of_n(policy_model, tokenizer, reward_model, sample_text, n=4, device=policy_model.device)\n",
    "\n",
    "print(f\"--- Best of 4 Selection (Score: {result['best_score']:.4f}) ---\")\n",
    "print(result['best_summary'])\n",
    "print(\"\\n--- All Candidates ---\")\n",
    "for i, (cand, score) in enumerate(zip(result['all_candidates'], result['all_scores'])):\n",
    "    print(f\"[{i+1}] Score: {score:.4f} | {cand}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65321497",
   "metadata": {},
   "source": [
    "## (Not Used) Attempt: Bias Mitigation with PPO\n",
    "We use the SFT model and fine-tune it to maximize the neutrality score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17360e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load Models\n",
    "sft_model_path = \"./sft_summarizer_final\"\n",
    "\n",
    "# Policy Model (The actor)\n",
    "policy_model = AutoModelForSeq2SeqLM.from_pretrained(sft_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(sft_model_path)\n",
    "\n",
    "# Reference Model (For KL divergence)\n",
    "ref_model = AutoModelForSeq2SeqLM.from_pretrained(sft_model_path)\n",
    "\n",
    "# Value Model (Critic)\n",
    "value_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    sft_model_path, \n",
    "    num_labels=1\n",
    ")\n",
    "\n",
    "# --- FIX GENERATION CONFIG ---\n",
    "# Explicitly update generation config to prevent \"min_length > max_length\" errors\n",
    "policy_model.generation_config.min_length = 30\n",
    "policy_model.generation_config.max_length = 1024 \n",
    "policy_model.generation_config.max_new_tokens = 128\n",
    "policy_model.generation_config.do_sample = True\n",
    "policy_model.generation_config.top_p = 0.9\n",
    "policy_model.generation_config.num_beams = 1\n",
    "policy_model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "policy_model.generation_config.decoder_start_token_id = tokenizer.eos_token_id\n",
    "policy_model.generation_config.forced_bos_token_id = 0\n",
    "policy_model.generation_config.early_stopping = False\n",
    "policy_model.config.use_cache = False\n",
    "\n",
    "# Sync ref_model config\n",
    "ref_model.generation_config = policy_model.generation_config\n",
    "ref_model.config.use_cache = False\n",
    "\n",
    "\n",
    "\n",
    "# 3. PPO Configuration\n",
    "config = PPOConfig(\n",
    "    output_dir=\"./unbiased_summarizer_ppo\",\n",
    "    learning_rate=1.41e-5,\n",
    "    batch_size=4,\n",
    "    mini_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_ppo_epochs=4,\n",
    "    seed=42,\n",
    "    # CRITICAL: Ensure generation parameters are set to prevent empty responses\n",
    "    response_length=128, # Max length of generated response\n",
    "    stop_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# 4. Prepare Dataset\n",
    "def tokenize(sample):\n",
    "    # Ensure truncation and padding are applied correctly\n",
    "    # Truncate to 512 to ensure efficiency and avoid position embedding limits\n",
    "    tokenized = tokenizer(sample[\"document\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "    sample[\"input_ids\"] = tokenized[\"input_ids\"]\n",
    "    sample[\"attention_mask\"] = tokenized[\"attention_mask\"]\n",
    "    return sample\n",
    "\n",
    "# Apply tokenization\n",
    "ppo_dataset = dataset_train.map(tokenize, batched=False)\n",
    "\n",
    "# Remove non-tensor columns\n",
    "cols_to_remove = [col for col in [\"document\", \"summary\", \"id\", \"__index_level_0__\"] if col in ppo_dataset.column_names]\n",
    "ppo_dataset = ppo_dataset.remove_columns(cols_to_remove)\n",
    "\n",
    "# Set format for PyTorch\n",
    "ppo_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "# 5. Initialize PPOTrainer\n",
    "# We need to pass generation_kwargs to ensure valid generation\n",
    "generation_kwargs = {\n",
    "    \"min_length\": 30,\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"num_beams\": 1,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"decoder_start_token_id\": tokenizer.eos_token_id, # Important for BART\n",
    "}\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    args=config,\n",
    "    processing_class=tokenizer,\n",
    "    model=policy_model,\n",
    "    ref_model=ref_model,\n",
    "    reward_model=reward_model,\n",
    "    value_model=value_model,\n",
    "    train_dataset=ppo_dataset,\n",
    ")\n",
    "\n",
    "# Set generation kwargs in the trainer\n",
    "ppo_trainer.generation_kwargs = generation_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2e7825",
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Debug: Try generation with Sampling + No Repeat N-Grams (PPO Style)\n",
    "sample_text = \"\"\"\n",
    "    Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to the natural intelligence displayed by animals including humans. AI research has been defined as the field of study of intelligent agents, which refers to any system that perceives its environment and takes actions that maximize its chance of achieving its goals.\n",
    "    The term \"artificial intelligence\" had previously been used to describe machines that mimic and display \"human\" cognitive skills that are associated with the human mind, such as \"learning\" and \"problem-solving\". This definition has since been rejected by major AI researchers who now describe AI in terms of rationality and acting rationally, which does not limit how intelligence can be articulated.\n",
    "    \"\"\"\n",
    "\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "\n",
    "print(\"\\n--- Attempt 2: Sampling with Constraints (PPO Style) ---\")\n",
    "summary_ids = policy_model.generate(\n",
    "    inputs[\"input_ids\"], \n",
    "    max_length=100, \n",
    "    min_length=30, \n",
    "    do_sample=True,              # Enable Sampling\n",
    "    num_beams=1,                 # Explicitly set to 1 for sampling\n",
    "    top_p=0.9,                   # Nucleus sampling\n",
    "    no_repeat_ngram_size=3,      # Prevent repetition\n",
    "    early_stopping=False         # Disable early_stopping for sampling\n",
    ")\n",
    "print(f\"IDs: {summary_ids[0].tolist()}\")\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"Generated Summary:\")\n",
    "print(summary)\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46e227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trl.trainer.ppo_trainer\n",
    "from transformers import GenerationConfig, BartForConditionalGeneration, BartForSequenceClassification, BartModel\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput, Seq2SeqModelOutput\n",
    "from trl import AutoModelForSeq2SeqLMWithValueHead, PPOTrainer\n",
    "import torch\n",
    "import types\n",
    "\n",
    "# --- FIX 7: MONKEY PATCH get_reward ---\n",
    "# TRL's get_reward tries to run the reward model backbone on the full sequence (640 tokens).\n",
    "# Our reward model (BERT) only supports 512.\n",
    "# We patch get_reward to use the custom forward method of NeutralityRewardModel, which handles truncation.\n",
    "print(\"Patching get_reward to handle NeutralityRewardModel truncation...\")\n",
    "\n",
    "if not hasattr(trl.trainer.ppo_trainer, \"original_get_reward\"):\n",
    "    trl.trainer.ppo_trainer.original_get_reward = trl.trainer.ppo_trainer.get_reward\n",
    "\n",
    "def custom_get_reward(model, query_responses, pad_token_id, context_length):\n",
    "    # Unwrap model to check type (in case of DataParallel/Accelerator wrapping)\n",
    "    unwrapped = model\n",
    "    while hasattr(unwrapped, \"module\"):\n",
    "        unwrapped = unwrapped.module\n",
    "        \n",
    "    if unwrapped.__class__.__name__ == \"NeutralityRewardModel\":\n",
    "        # Custom handling for our reward model\n",
    "        device = next(unwrapped.parameters()).device\n",
    "        query_responses = query_responses.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # The forward method of NeutralityRewardModel handles decoding and truncation\n",
    "            # It returns a tensor of rewards\n",
    "            scores = unwrapped(input_ids=query_responses)\n",
    "        \n",
    "        # get_reward returns (values, score, sequence_lengths)\n",
    "        # We only need score for the reward model\n",
    "        return None, scores, None\n",
    "        \n",
    "    return trl.trainer.ppo_trainer.original_get_reward(model, query_responses, pad_token_id, context_length)\n",
    "\n",
    "trl.trainer.ppo_trainer.get_reward = custom_get_reward\n",
    "\n",
    "# --- FIX 5: MONKEY PATCH Seq2Seq Output Classes ---\n",
    "# TRL expects 'hidden_states' but Seq2Seq models return 'decoder_hidden_states'\n",
    "print(\"Patching Seq2Seq output classes to expose hidden_states...\")\n",
    "def get_hidden_states(self):\n",
    "    return self.decoder_hidden_states\n",
    "\n",
    "if not hasattr(Seq2SeqLMOutput, \"hidden_states\"):\n",
    "    Seq2SeqLMOutput.hidden_states = property(get_hidden_states)\n",
    "\n",
    "if not hasattr(Seq2SeqModelOutput, \"hidden_states\"):\n",
    "    Seq2SeqModelOutput.hidden_states = property(get_hidden_states)\n",
    "\n",
    "# --- FIX 4: MONKEY PATCH AutoModelForSeq2SeqLMWithValueHead ---\n",
    "print(\"Patching AutoModelForSeq2SeqLMWithValueHead to expose base_model_prefix, model, and score...\")\n",
    "if not hasattr(AutoModelForSeq2SeqLMWithValueHead, \"base_model_prefix\"):\n",
    "    def get_base_model_prefix(self):\n",
    "        return self.pretrained_model.base_model_prefix\n",
    "    AutoModelForSeq2SeqLMWithValueHead.base_model_prefix = property(get_base_model_prefix)\n",
    "\n",
    "if not hasattr(AutoModelForSeq2SeqLMWithValueHead, \"model\"):\n",
    "    def get_model(self):\n",
    "        if hasattr(self.pretrained_model, \"model\"):\n",
    "            return self.pretrained_model.model\n",
    "        return self.pretrained_model\n",
    "    AutoModelForSeq2SeqLMWithValueHead.model = property(get_model)\n",
    "\n",
    "if not hasattr(AutoModelForSeq2SeqLMWithValueHead, \"score\"):\n",
    "    def get_score(self):\n",
    "        if hasattr(self, \"v_head\"):\n",
    "            return self.v_head\n",
    "        if hasattr(self.pretrained_model, \"score\"):\n",
    "            return self.pretrained_model.score\n",
    "        return None\n",
    "    AutoModelForSeq2SeqLMWithValueHead.score = property(get_score)\n",
    "\n",
    "# --- FIX 6: PATCH REWARD MODEL ---\n",
    "# The custom NeutralityRewardModel doesn't have base_model_prefix, which TRL might check.\n",
    "print(\"Patching Reward Model to expose base_model_prefix and underlying model...\")\n",
    "if \"reward_model\" in globals():\n",
    "    # 1. Ensure base_model_prefix exists\n",
    "    if not hasattr(reward_model, \"base_model_prefix\"):\n",
    "        if hasattr(reward_model, \"reward_model\") and hasattr(reward_model.reward_model, \"base_model_prefix\"):\n",
    "             reward_model.base_model_prefix = reward_model.reward_model.base_model_prefix\n",
    "        else:\n",
    "             reward_model.base_model_prefix = \"model\"\n",
    "    \n",
    "    # 2. Ensure the attribute named by base_model_prefix exists (e.g. 'bert')\n",
    "    # TRL tries to access model.bert if base_model_prefix is 'bert'\n",
    "    prefix = reward_model.base_model_prefix\n",
    "    RewardModelClass = type(reward_model)\n",
    "    \n",
    "    if not hasattr(RewardModelClass, prefix):\n",
    "        print(f\"Patching {RewardModelClass.__name__} to expose '{prefix}'...\")\n",
    "        def get_inner_base_model(self):\n",
    "            return getattr(self.reward_model, prefix)\n",
    "        setattr(RewardModelClass, prefix, property(get_inner_base_model))\n",
    "\n",
    "# --- FIX 3: RE-INITIALIZE VALUE MODEL & TRAINER ---\n",
    "print(\"Re-initializing Value Model and PPO Trainer...\")\n",
    "\n",
    "if \"sft_model_path\" not in globals():\n",
    "    sft_model_path = \"./sft_summarizer_final\" \n",
    "\n",
    "value_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(sft_model_path)\n",
    "print(f\"[DEBUG] Value Model Type: {type(value_model)}\")\n",
    "print(f\"[DEBUG] Value Model Attributes: v_head={hasattr(value_model, 'v_head')}, score={hasattr(value_model, 'score')}\")\n",
    "\n",
    "if hasattr(policy_model, \"device\"):\n",
    "    value_model.to(policy_model.device)\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    args=config,\n",
    "    processing_class=tokenizer,\n",
    "    model=policy_model,\n",
    "    ref_model=ref_model,\n",
    "    reward_model=reward_model,\n",
    "    value_model=value_model,\n",
    "    train_dataset=ppo_dataset,\n",
    ")\n",
    "ppo_trainer.generation_kwargs = generation_kwargs\n",
    "print(f\"[DEBUG] PPOTrainer Value Model Type: {type(ppo_trainer.value_model)}\")\n",
    "\n",
    "# --- MONKEY PATCH 1: FORCE GENERATION CONFIG & FIX SEQ2SEQ HANDLING ---\n",
    "if not hasattr(trl.trainer.ppo_trainer, \"original_batch_generation\"):\n",
    "    trl.trainer.ppo_trainer.original_batch_generation = trl.trainer.ppo_trainer.batch_generation\n",
    "\n",
    "def custom_batch_generation(policy, queries, local_rollout_forward_batch_size, pad_token_id, generation_config):\n",
    "    # 1. FORCE CONFIGURATION\n",
    "    new_config = GenerationConfig.from_dict(generation_config.to_dict())\n",
    "    new_config.min_length = 30\n",
    "    new_config.max_new_tokens = 128\n",
    "    new_config.do_sample = True\n",
    "    new_config.top_p = 0.9\n",
    "    new_config.num_beams = 1\n",
    "    new_config.decoder_start_token_id = 2 \n",
    "    new_config.eos_token_id = 2\n",
    "    new_config.pad_token_id = 1\n",
    "    \n",
    "    # 2. MANUAL GENERATION LOOP\n",
    "    query_responses = []\n",
    "    logitss = []\n",
    "    batch_size = queries.shape[0]\n",
    "    \n",
    "    for i in range(0, batch_size, local_rollout_forward_batch_size):\n",
    "        query = queries[i : i + local_rollout_forward_batch_size]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = policy.generate(\n",
    "                input_ids=query,\n",
    "                attention_mask=(query != pad_token_id).long(),\n",
    "                generation_config=new_config,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "            )\n",
    "            \n",
    "        logits = torch.stack(output.scores, 1)\n",
    "        \n",
    "        if policy.config.is_encoder_decoder:\n",
    "            generated_part = output.sequences\n",
    "            gen_len = generated_part.shape[1]\n",
    "            logits_len = logits.shape[1]\n",
    "            \n",
    "            if gen_len > logits_len:\n",
    "                generated_part = generated_part[:, -logits_len:]\n",
    "            elif logits_len > gen_len:\n",
    "                logits = logits[:, :gen_len, :]\n",
    "                \n",
    "            full_sequence = torch.cat((query, generated_part), dim=1)\n",
    "        else:\n",
    "            full_sequence = output.sequences\n",
    "            \n",
    "        query_responses.append(full_sequence)\n",
    "        logitss.append(logits)\n",
    "        \n",
    "        if i == 0:\n",
    "            print(f\"\\n[DEBUG] Model Type: {'Encoder-Decoder' if policy.config.is_encoder_decoder else 'Decoder-Only'}\")\n",
    "            print(f\"[DEBUG] Query Shape: {query.shape}\")\n",
    "            print(f\"[DEBUG] Output Sequences Shape: {output.sequences.shape}\")\n",
    "            print(f\"[DEBUG] Logits Shape: {logits.shape}\")\n",
    "            print(f\"[DEBUG] Full Sequence Shape: {full_sequence.shape}\")\n",
    "\n",
    "    return torch.cat(query_responses, 0), torch.cat(logitss, 0)\n",
    "\n",
    "trl.trainer.ppo_trainer.batch_generation = custom_batch_generation\n",
    "print(\"Monkey-patch applied: Fixed Seq2Seq slicing logic + Enforced Config + Logit/Seq Length Mismatch Fix\")\n",
    "\n",
    "# --- MONKEY PATCH 2: ROBUST INSTANCE-LEVEL FORWARD PATCH ---\n",
    "def make_safe_forward(original_forward, model_name=\"Model\"):\n",
    "    def safe_forward(self, *args, **kwargs):\n",
    "        if \"position_ids\" in kwargs:\n",
    "            kwargs.pop(\"position_ids\")\n",
    "        return original_forward(*args, **kwargs)\n",
    "    return types.MethodType(safe_forward, original_forward.__self__)\n",
    "\n",
    "def patch_bart_instances(module, name=\"\"):\n",
    "    if isinstance(module, (BartForConditionalGeneration, BartForSequenceClassification, BartModel)):\n",
    "        if not getattr(module, \"_forward_is_patched\", False):\n",
    "            print(f\"Patching forward method of: {name} ({type(module).__name__})\")\n",
    "            module.forward = make_safe_forward(module.forward, name)\n",
    "            module._forward_is_patched = True\n",
    "    \n",
    "    for child_name, child in module.named_children():\n",
    "        patch_bart_instances(child, f\"{name}.{child_name}\")\n",
    "\n",
    "print(\"Applying instance-level patches...\")\n",
    "if hasattr(ppo_trainer.model, \"pretrained_model\"):\n",
    "    patch_bart_instances(ppo_trainer.model.pretrained_model, \"ppo_trainer.model.pretrained_model\")\n",
    "else:\n",
    "    patch_bart_instances(ppo_trainer.model, \"ppo_trainer.model\")\n",
    "\n",
    "patch_bart_instances(ref_model, \"ref_model\")\n",
    "\n",
    "if hasattr(value_model, \"pretrained_model\"):\n",
    "    patch_bart_instances(value_model.pretrained_model, \"value_model.pretrained_model\")\n",
    "else:\n",
    "    patch_bart_instances(value_model, \"value_model\")\n",
    "\n",
    "patch_bart_instances(reward_model.reward_model, \"reward_model\")\n",
    "\n",
    "# --- DEBUG: INSPECT MODEL CONFIGS ---\n",
    "print(f\"Policy Model is_encoder_decoder: {policy_model.config.is_encoder_decoder}\")\n",
    "\n",
    "# 5. Start Training\n",
    "print(\"Starting PPO Training...\")\n",
    "ppo_trainer.is_encoder_decoder = True\n",
    "print(f\"Forced ppo_trainer.is_encoder_decoder = {ppo_trainer.is_encoder_decoder}\")\n",
    "\n",
    "ppo_trainer.train()\n",
    "ppo_trainer.save_model(\"./unbiased_summarizer_ppo_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5928b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".arc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

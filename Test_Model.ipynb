{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a113f2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tmittal/CSE595/.arc_env/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking model at: ./unbiased_summarizer_dpo_final_1000\n",
      "Using device: cuda\n",
      "Loading tokenizer...\n",
      "Special Tokens: BOS=0, EOS=2, PAD=1\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tmittal/CSE595/.arc_env/lib64/python3.12/site-packages/transformers/models/bart/configuration_bart.py:177: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "!!! Model Verification Failed !!!\n",
      "Error: CUDA error: CUDA-capable device(s) is/are busy or unavailable\n",
      "Search for `cudaErrorDevicesUnavailable' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3633966/3012275925.py\", line 22, in <module>\n",
      "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tmittal/CSE595/.arc_env/lib64/python3.12/site-packages/transformers/modeling_utils.py\", line 4343, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tmittal/CSE595/.arc_env/lib64/python3.12/site-packages/torch/nn/modules/module.py\", line 1371, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tmittal/CSE595/.arc_env/lib64/python3.12/site-packages/torch/nn/modules/module.py\", line 930, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/tmittal/CSE595/.arc_env/lib64/python3.12/site-packages/torch/nn/modules/module.py\", line 930, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/tmittal/CSE595/.arc_env/lib64/python3.12/site-packages/torch/nn/modules/module.py\", line 957, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/tmittal/CSE595/.arc_env/lib64/python3.12/site-packages/torch/nn/modules/module.py\", line 1357, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.AcceleratorError: CUDA error: CUDA-capable device(s) is/are busy or unavailable\n",
      "Search for `cudaErrorDevicesUnavailable' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "import traceback\n",
    "\n",
    "# Path to your saved model\n",
    "model_path = \"./unbiased_summarizer_dpo_final_1000\"\n",
    "\n",
    "print(f\"Checking model at: {model_path}\")\n",
    "\n",
    "try:\n",
    "    # Determine device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load Tokenizer\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    print(f\"Special Tokens: BOS={tokenizer.bos_token_id}, EOS={tokenizer.eos_token_id}, PAD={tokenizer.pad_token_id}\")\n",
    "    \n",
    "    # Load Model\n",
    "    print(\"Loading model...\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "    print(\"Model loaded successfully!\")\n",
    "    \n",
    "    # --- FIX CONFIGURATION ISSUES GLOBALLY ---\n",
    "    # Explicitly set config values to prevent \"NoneType\" and \"early_stopping\" errors\n",
    "    model.config.early_stopping = True\n",
    "    model.config.forced_bos_token_id = 0\n",
    "    if model.config.decoder_start_token_id is None:\n",
    "        model.config.decoder_start_token_id = model.config.eos_token_id\n",
    "    \n",
    "    # Check for NaN weights (sign of corrupted training)\n",
    "    print(\"Checking model weights...\")\n",
    "    for name, param in list(model.named_parameters())[:5]:\n",
    "        if torch.isnan(param).any():\n",
    "            print(f\"WARNING: Parameter {name} contains NaNs!\")\n",
    "        if param.sum() == 0:\n",
    "             print(f\"WARNING: Parameter {name} is all zeros!\")\n",
    "\n",
    "    # Test Generation\n",
    "    sample_text = \"\"\"\n",
    "    Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to the natural intelligence displayed by animals including humans. AI research has been defined as the field of study of intelligent agents, which refers to any system that perceives its environment and takes actions that maximize its chance of achieving its goals.\n",
    "    The term \"artificial intelligence\" had previously been used to describe machines that mimic and display \"human\" cognitive skills that are associated with the human mind, such as \"learning\" and \"problem-solving\". This definition has since been rejected by major AI researchers who now describe AI in terms of rationality and acting rationally, which does not limit how intelligence can be articulated.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nGenerating test summary...\")\n",
    "    inputs = tokenizer(sample_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "    \n",
    "    # Debug: Try generation with minimal constraints\n",
    "    print(\"\\n--- Attempt 1: Greedy Search (No Forcing) ---\")\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_length=100, \n",
    "        min_length=10, \n",
    "        do_sample=False, \n",
    "        num_beams=1\n",
    "    )\n",
    "    print(f\"IDs: {summary_ids[0].tolist()}\")\n",
    "    print(f\"Text: {tokenizer.decode(summary_ids[0], skip_special_tokens=True)}\")\n",
    "\n",
    "    # Debug: Try generation with Sampling + No Repeat N-Grams (PPO Style)\n",
    "    print(\"\\n--- Attempt 2: Sampling with Constraints (PPO Style) ---\")\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_length=100, \n",
    "        min_length=30, \n",
    "        do_sample=True,              # Enable Sampling\n",
    "        num_beams=1,                 # Explicitly set to 1 for sampling\n",
    "        top_p=0.9,                   # Nucleus sampling\n",
    "        no_repeat_ngram_size=3,      # Prevent repetition\n",
    "        early_stopping=False         # Disable early_stopping for sampling\n",
    "    )\n",
    "    print(f\"IDs: {summary_ids[0].tolist()}\")\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(\"Generated Summary:\")\n",
    "    print(summary)\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    if not summary.strip():\n",
    "        print(\"WARNING: Summary is empty. The model might be generating only special tokens.\")\n",
    "    else:\n",
    "        print(\"Verification Complete: Model is functional.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\n!!! Model Verification Failed !!!\")\n",
    "    print(f\"Error: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55653d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".arc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

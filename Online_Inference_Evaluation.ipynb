{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b6fd3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tmittal/CSE595/.arc_env/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 2. Imports\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bert_score import score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8a2740d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path set to: ./unbiased_summarizer_dpo_final_1000\n"
     ]
    }
   ],
   "source": [
    "# 3. Configuration\n",
    "model_path = \"./unbiased_summarizer_dpo_final_1000\"\n",
    "print(f\"Model path set to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0f620b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BestOfNModel...\n",
      "Loading Policy Model from ./unbiased_summarizer_dpo_final_1000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tmittal/CSE595/.arc_env/lib64/python3.12/site-packages/transformers/models/bart/configuration_bart.py:177: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Reward Model: maximuspowers/bias-type-classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 4. Initialize BestOfNModel\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ensure we can import from the current directory\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.getcwd())\n",
    "\n",
    "from bestofn import BestOfNModel\n",
    "\n",
    "# Initialize the model wrapper\n",
    "# This handles loading the policy model, tokenizer, and reward model\n",
    "print(\"Initializing BestOfNModel...\")\n",
    "best_of_n_model = BestOfNModel(model_path, n=4, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f87ce9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Multi-News Test Set...\n",
      "Evaluating on 100 samples.\n"
     ]
    }
   ],
   "source": [
    "# 5. Load Dataset\n",
    "print(\"Loading Multi-News Test Set...\")\n",
    "dataset = load_dataset(\"Awesome075/multi_news_parquet\", split=\"test\")\n",
    "\n",
    "# Select subset for faster evaluation (e.g., 50 samples)\n",
    "num_samples = 100\n",
    "test_dataset = dataset.select(range(min(len(dataset), num_samples)))\n",
    "print(f\"Evaluating on {len(test_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d0a0a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries using Best-of-4 strategy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/home/tmittal/CSE595/.arc_env/lib64/python3.12/site-packages/transformers/generation/utils.py:1733: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "100%|██████████| 100/100 [01:46<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# 6. Evaluation Loop (Best-of-N)\n",
    "generated_summaries = []\n",
    "reference_summaries = []\n",
    "documents = []\n",
    "neutrality_scores = []\n",
    "\n",
    "print(f\"Generating summaries using Best-of-{best_of_n_model.n} strategy...\")\n",
    "\n",
    "# Iterate one by one since the class handles single inputs\n",
    "for i in tqdm(range(len(test_dataset))):\n",
    "    sample = test_dataset[i]\n",
    "    doc = sample[\"document\"]\n",
    "    ref = sample[\"summary\"]\n",
    "    \n",
    "    # Use the class to generate\n",
    "    # The forward method returns a dict with 'best_summary', 'best_score', etc.\n",
    "    result = best_of_n_model.forward(doc)\n",
    "    \n",
    "    generated_summaries.append(result[\"best_summary\"])\n",
    "    reference_summaries.append(ref)\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f056cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Neutrality Score: 0.5509\n",
      "Computing BERTScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 191.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.77 seconds, 56.63 sentences/sec\n",
      "BERTScore F1: 0.8503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 7. Compute Metrics\n",
    "bias_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"cirimus/modernbert-large-bias-type-classifier\",\n",
    "    top_k=None,\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "def compute_neutrality(texts):\n",
    "    bias_outputs = bias_classifier(texts, batch_size=8, truncation=True, max_length=512)\n",
    "    neutrality_scores = []\n",
    "    for output in bias_outputs:\n",
    "        scores = sorted([float(item[\"score\"]) for item in output], reverse=True)\n",
    "        top3 = scores[:3] if len(scores) >= 3 else scores\n",
    "        avg_top3 = np.mean(top3)\n",
    "        neutrality = (1.0 - avg_top3) ** 2\n",
    "        neutrality_scores.append(neutrality)\n",
    "    return neutrality_scores\n",
    "\n",
    "neutrality_scores = compute_neutrality(generated_summaries)\n",
    "print(f\"Average Neutrality Score: {np.mean(neutrality_scores):.4f}\")\n",
    "\n",
    "print(\"Computing BERTScore...\")\n",
    "P, R, F1 = score(generated_summaries, reference_summaries, lang=\"en\", verbose=True, device=device, batch_size=16)\n",
    "print(f\"BERTScore F1: {F1.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9196b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Save Results\n",
    "df = pd.DataFrame({\n",
    "    \"document\": documents,\n",
    "    \"reference\": reference_summaries,\n",
    "    \"generated\": generated_summaries,\n",
    "    \"neutrality_score\": neutrality_scores,\n",
    "    \"bert_score_f1\": F1.tolist()\n",
    "})\n",
    "output_file = \"online_inference_results.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Detailed results saved to {output_file}\")\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".arc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
